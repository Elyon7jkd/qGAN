
!pip install pennylane --upgrade

!pip install pennylane custatevec-cu11 pennylane-lightning-gpu

# Library imports
import math
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import pennylane as qml

# Pytorch imports
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import torchvision.datasets as dset
import torchvision.utils as vutils
# Set the random seed for reproducibility
seed = 300
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

qml.about()

from google.colab import drive
drive.mount('/content/drive')

# Number of workers for dataloader
workers = 2

# Batch size during training
batch_size = 32

# Spatial size of training images. All images will be resized to this
#   size using a transformer.
image_size = 64

# Number of GPUs available. Use 0 for CPU mode.
ngpu = 1

lrG = 0.3  # Learning rate for the generator
lrD = 0.01  # Learning rate for the discriminator
num_iter = 100# Number of training iterations

device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")
print(device)

ImgLocation='/content/drive/MyDrive/Colab Notebooks/Subset_Dil_Bos/'

dataset = dset.ImageFolder(root=ImgLocation,
                           transform=transforms.Compose([
                               transforms.Resize(image_size),
                               transforms.CenterCrop(image_size),
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                           ]))

dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=workers)

# Plot some training images
real_batch = next(iter(dataloader))
plt.figure(figsize=(8,8))
plt.axis("off")
plt.title("Training Images")
plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))
#plt.imshow(real_batch)

class Discriminator(nn.Module):
    """Fully connected classical discriminator"""

    def __init__(self):
        super().__init__()
        self.ngpu = ngpu
        self.model = nn.Sequential(
            # Inputs to first hidden layer (num_input_features -> 64)
            nn.Linear(3*image_size * image_size, 64,device=device),
            nn.ReLU(),
            nn.Linear(64, 64,device=device),
            nn.ReLU(),
            # First hidden layer (64 -> 16)
            nn.Linear(64, 16,device=device),
            nn.ReLU(),
            # Second hidden layer (16 -> output)
            nn.Linear(16, 1,device=device),
            nn.Sigmoid(),
        )

    def forward(self, x):
        return self.model(x)

# Quantum variables
n_qubits = 9  # Total number of qubits / N
n_a_qubits = 1  # Number of ancillary qubits / N_A
q_depth = 5  # Depth of the parameterised quantum circuit / D
n_generators = 48  # Number of subgenerators for the patch method / N_G

# Quantum simulator

#dev = qml.device("default.qubit", wires=n_qubits)
#dev = qml.device("lightning.gpu", wires=n_qubits)


dev = qml.device("lightning.qubit", wires=n_qubits)
print(dev)

@qml.qnode(dev, interface="torch") #, diff_method="parameter-shift"
def quantum_circuit(noise, weights):

    weights = weights.reshape(q_depth, n_qubits)

    # Initialise latent vectors
    for i in range(n_qubits):
        qml.RY(noise[i], wires=i)

    # Repeated layer
    for i in range(q_depth):
        # Parameterised layer
        for y in range(n_qubits):
            qml.RY(weights[i][y], wires=y)

        # Control Z gates
        for y in range(n_qubits - 1):
            qml.CZ(wires=[y, y + 1])

    return qml.probs(wires=list(range(n_qubits)))


def partial_measure(noise, weights):
    # Non-linear Transform
    probs = quantum_circuit(noise, weights)
    probsgiven0 = probs[: (2 ** (n_qubits - n_a_qubits))]
    probsgiven0 /= torch.sum(probs)

    # Post-Processing
    probsgiven = probsgiven0 / torch.max(probsgiven0)
    return probsgiven

class PatchQuantumGenerator(nn.Module):
    """Quantum generator class for the patch method"""

    def __init__(self, n_generators, q_delta=1):
        """
        Args:
            n_generators (int): Number of sub-generators to be used in the patch method.
            q_delta (float, optional): Spread of the random distribution for parameter initialisation.
        """

        super().__init__()

        self.q_params = nn.ParameterList(
            [
                nn.Parameter(q_delta * torch.rand(q_depth * n_qubits), requires_grad=True)
                for _ in range(n_generators)
            ]
        )
        self.n_generators = n_generators

    def forward(self, x):
        # Size of each sub-generator output
        patch_size = 2 ** (n_qubits - n_a_qubits)

        # Create a Tensor to 'catch' a batch of images from the for loop. x.size(0) is the batch size.
        images = torch.Tensor(x.size(0), 0).to(device)

        # Iterate over all sub-generators
        for params in self.q_params:

            # Create a Tensor to 'catch' a batch of the patches from a single sub-generator
            patches = torch.Tensor(0, patch_size).to(device)
            for elem in x:
                q_out = partial_measure(elem, params).float().unsqueeze(0)
                patches = torch.cat((patches, q_out))

            # Each batch of patches is concatenated with each other to create a batch of images
            images = torch.cat((images, patches), 1)

        return images

"""Training
========

"""

discriminator = Discriminator(ngpu).to(device)

generator = PatchQuantumGenerator(n_generators).to(device)

print(discriminator)

print(generator)

# Binary cross entropy
criterion = nn.BCELoss()

# Optimisers
optD = optim.SGD(discriminator.parameters(), lr=lrG)
optG = optim.SGD(generator.parameters(), lr=lrD)

real_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)
fake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)

# Fixed noise allows us to visually track the generated images throughout training
fixed_noise = torch.rand(batch_size, n_qubits, device=device) * math.pi / 2

save_path='/content/drive/MyDrive/Colab Notebooks/quantum_mod.pt'

def save_checkpoint(gen,dis, optg,optd, save_path):
    torch.save({
        'gen_state_dict': gen.state_dict(),
        'dis_state_dict': dis.state_dict(),
        'optG_state_dict': optg.state_dict(),
        'optD_state_dict': optd.state_dict()

    }, save_path)
def load_checkpoint(gen,dis, optg,optd, load_path):
    checkpoint = torch.load(load_path)
    gen.load_state_dict(checkpoint['gen_state_dict'])
    dis.load_state_dict(checkpoint['dis_state_dict'])
    optg.load_state_dict(checkpoint['optG_state_dict'])
    optd.load_state_dict(checkpoint['optD_state_dict'])


    return gen,dis, optg, optd

# Iteration counter
counter = 0
G_losses = []
D_losses = []
# Collect images for plotting later
results = []

while True:
    for i, (data, _) in enumerate(dataloader):
        # Data for training the discriminator
        data = data.reshape( -1,image_size * image_size*3) #data=(32,12288)
        real_data = data.to(device) #real_data=(32,12288)
        b_size = real_data.size(0) #32
        real_labels = torch.full((b_size,), 1.0, dtype=torch.float, device=device)
        fake_labels = torch.full((b_size,), 0.0, dtype=torch.float, device=device)
        # Training the discriminator
        # Noise follwing a uniform distribution in range [0,pi/2)
        noise = torch.rand(batch_size, n_qubits, device=device) * math.pi / 2 #noise=(32,13)
        fake_data = generator(noise)  #fake_data=(32,12288)
        discriminator.zero_grad()
        outD_real = discriminator(real_data).view(-1) #(outD_real=32)
        errD_real = criterion(outD_real, real_labels)  #(criterion(32, 32))
        outD_fake = discriminator(fake_data.detach()).view(-1) #(outD_fake=32)
        errD_fake = criterion(outD_fake, fake_labels)  #(criterion(32,32))
        # Propagate gradients
        errD_real.backward()
        errD_fake.backward()
        errD = errD_real + errD_fake
        optD.step()
        # Training the generator
        generator.zero_grad()
        outD_fake = discriminator(fake_data).view(-1)  #outD_fake=32
        errG = criterion(outD_fake, real_labels) #criterion(32,32)
        errG.backward()
        optG.step()
        counter += 1
        G_losses.append(errG.item())
        D_losses.append(errD.item())
        # Show loss values
        if counter % 10 == 0:
            print(f'Iteration: {counter}, Discriminator Loss: {errD:0.3f}, Generator Loss: {errG:0.3f}')
            test_images = generator(fixed_noise).view(batch_size,3,image_size,image_size).cpu().detach()

            # Save images every 50 iterations
            if counter % 50 == 0:
                results.append(test_images)

        if counter == num_iter:
            break
    if counter == num_iter:
        break

save_checkpoint(generator,discriminator, optG,optD, save_path)

load_checkpoint(generator,discriminator, optG,optD, save_path)

plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(G_losses,label="G")
plt.plot(D_losses,label="D")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

for k in range(len(test_images)) :

      fig, axs = plt.subplots(1, 1, sharey=False, tight_layout=True, figsize=(2,2), facecolor='white')


      axs.matshow(np.squeeze(test_images[k].permute(1,2,0)))
